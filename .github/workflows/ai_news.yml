name: AI Daily Brief for Amazon JP & Japanese Teaching

on:
  schedule:
    # æ¯å¤©ä¸œäº¬æ—¶é—´ 08:30 æ¨é€ï¼ˆGitHub Actions ç”¨ UTCï¼›08:30 JST = 23:30 UTC å‰ä¸€å¤©ï¼‰
    - cron: "30 23 * * *"
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: ai-daily-brief
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -U feedparser requests beautifulsoup4

      - name: Run AI Daily Brief Bot
        env:
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
          WECHAT_WEBHOOK: ${{ secrets.WECHAT_WEBHOOK }}
        run: |
          python - <<'EOF'
          import feedparser
          import requests
          import os, sys, json, re, time, hashlib
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone

          webhook = os.environ.get("WECHAT_WEBHOOK")
          api_key = os.environ.get("AI_API_KEY")
          if not webhook or not api_key:
              print("é”™è¯¯: ç¼ºå°‘é…ç½®å˜é‡ WECHAT_WEBHOOK æˆ– AI_API_KEY")
              sys.exit(1)

          # ====== ä½ çš„èº«ä»½ä¸åå¥½ï¼ˆä½  + ä½ è€å©†ï¼‰======
          # 1) æ—¥äºšè·¨å¢ƒç”µå•†/è¿è¥å¢é•¿/åˆè§„/å·¥å…·è½åœ°
          W_ECOM = 1.35
          # 2) æ—¥è¯­æ•™å¸ˆï¼šAIæ•™å­¦ã€è¯­æ–™åº“ã€æ•™å­¦èµ„æºã€ç ”ç©¶æ–¹æ³•
          W_TEACH = 1.15
          # 3) é€šç”¨ AI å·¥å…·/æ¡†æ¶ï¼šåªä¿ç•™â€œå¯è½åœ°â€å†…å®¹
          W_GENERAL = 1.00

          # ====== RSS æºï¼ˆåå·¥å…·ã€æ•™ç¨‹ã€å‘å¸ƒã€å®æ“ï¼‰======
          SOURCES = [
              # å®æ“/è¯¾ç¨‹ï¼ˆAndrew Ng çš„ The Batchï¼‰
              "https://rsshub.app/deeplearning/the-batch",
              # Hugging Face åšå®¢ï¼ˆå¼€æº/æ¨¡å‹/å·¥å…·ï¼‰
              "https://huggingface.co/blog/feed.xml",
              # LangChain åšå®¢ï¼ˆAgent/Workflow/RAGï¼‰
              "https://blog.langchain.dev/rss/",
              # Unite.ai å·¥å…·ç±»ï¼ˆåæ³›ï¼Œç•™ç€ç”¨åˆ†æ•°ç­›ï¼‰
              "https://www.unite.ai/category/ai-tools/feed/",
              # OpenAI å®˜æ–¹æ›´æ–°ï¼ˆå·¥å…·/èƒ½åŠ›å˜æ›´ï¼šæ›´ç¨³ï¼‰
              "https://openai.com/blog/rss.xml",
              # Google AIï¼ˆæ¨¡å‹ã€å·¥å…·ã€ç ”ç©¶åˆ°äº§å“ï¼‰
              "https://blog.google/technology/ai/rss/",
              # AWS æœºå™¨å­¦ä¹ ï¼ˆå¯¹ç”µå•†åŸºç¡€è®¾æ–½ä¸è½åœ°æœ‰ç”¨ï¼‰
              "https://aws.amazon.com/blogs/machine-learning/feed/",
          ]

          HEADERS = {
              "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
          }

          # ====== å…³é”®è¯ç”»åƒï¼ˆå°‘è€Œå‡†ï¼Œé¿å…å™ªéŸ³ï¼‰======
          ECOM_KEYWORDS = [
              "amazon", "fba", "seller", "listing", "keyword", "ads", "ppc", "sponsored",
              "search term", "ranking", "seo", "conversion", "cvr", "ctr", "acos", "tacos",
              "pricing", "review", "a+", "brand", "variation", "compliance", "policy",
              "automation", "scrape", "api", "rapidapi", "rainforest", "catalog", "asin",
              "japan", "jp", "amazon.co.jp", "logistics", "inventory", "demand", "market research",
          ]
          TEACH_KEYWORDS = [
              "japanese", "japan", "jlpt", "n1", "n2", "grammar", "linguistics", "corpus",
              "teaching", "pedagogy", "classroom", "lesson", "assessment", "rubric",
              "anki", "quizlet", "spaced repetition",
              "llm", "ai", "chatgpt", "prompt", "generation", "feedback", "evaluation",
          ]
          GENERAL_KEYWORDS = [
              "agent", "workflow", "rag", "retrieval", "vector", "embedding", "eval", "benchmark",
              "open source", "release", "library", "framework", "tool", "sdk", "api",
              "tutorial", "guide", "course", "cookbook", "playbook",
              "langchain", "dify", "prompt", "vision", "multimodal",
          ]

          # ä¸æƒ³çœ‹çš„ï¼ˆæ³›èèµ„/å¹¶è´­/æ‹›è˜ç­‰ï¼‰
          NEGATIVE_KEYWORDS = [
              "funding", "raises", "series a", "series b", "acquired", "acquisition",
              "hiring", "we're hiring", "job", "career", "earnings",
          ]

          # ====== å†å²å»é‡ï¼ˆä¿è¯æ¯å¤©ä¸ä¸€æ ·ï¼‰======
          HISTORY_FILE = "sent_history.json"
          MAX_HISTORY = 800

          def load_history():
              if not os.path.exists(HISTORY_FILE):
                  return set()
              try:
                  with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                      data = json.load(f)
                  if isinstance(data, dict):
                      links = data.get("links", [])
                  elif isinstance(data, list):
                      links = data
                  else:
                      links = []
                  return set(links)
              except Exception:
                  return set()

          def save_history(sent_links):
              # ä¿åºï¼šç”¨ list æˆªæ–­åˆ°æœ€è¿‘ MAX_HISTORY
              links = list(sent_links)
              data = {"links": links[-MAX_HISTORY:]}
              with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False, indent=2)

          def clean_text(html):
              return BeautifulSoup(html or "", "html.parser").get_text(" ", strip=True)

          def normalize(s):
              s = (s or "").lower()
              s = re.sub(r"\s+", " ", s).strip()
              return s

          def contains_any(text, kws):
              return any(kw in text for kw in kws)

          def score_entry(title, summary, link, source_url):
              text = normalize((title or "") + " " + (summary or ""))

              # è´Ÿå‘è¿‡æ»¤å¼ºæ‰£åˆ†
              score = 0.0
              for nk in NEGATIVE_KEYWORDS:
                  if nk in text:
                      score -= 8

              # ä¸‰ä¸ªç»´åº¦åˆ†åˆ«è®¡åˆ†
              e = sum(1 for kw in ECOM_KEYWORDS if kw in text)
              t = sum(1 for kw in TEACH_KEYWORDS if kw in text)
              g = sum(1 for kw in GENERAL_KEYWORDS if kw in text)

              # æ ‡é¢˜æ›´é‡è¦ï¼šæ ‡é¢˜å‘½ä¸­é¢å¤–åŠ æƒ
              title_text = normalize(title or "")
              e_title = sum(1 for kw in ECOM_KEYWORDS if kw in title_text)
              t_title = sum(1 for kw in TEACH_KEYWORDS if kw in title_text)
              g_title = sum(1 for kw in GENERAL_KEYWORDS if kw in title_text)

              score += W_ECOM * (e * 2.2 + e_title * 2.0)
              score += W_TEACH * (t * 1.7 + t_title * 1.6)
              score += W_GENERAL * (g * 1.4 + g_title * 1.3)

              # å½¢æ€åå¥½ï¼šæ›´å€¾å‘å¯è½åœ°èµ„æº
              link_l = (link or "").lower()
              if any(x in link_l for x in ["github.com", "/docs", "cookbook", "tutorial", "guide", "release", "changelog"]):
                  score += 2.0

              # æºçš„è½»åº¦åŠ æƒï¼ˆä½ å¸¸ç”¨/æ›´é è°±çš„ï¼‰
              if "huggingface.co" in source_url:
                  score += 1.5
              if "langchain.dev" in source_url:
                  score += 1.5
              if "openai.com" in source_url:
                  score += 1.2
              if "deeplearning" in source_url:
                  score += 1.0
              if "aws.amazon.com" in source_url:
                  score += 0.8

              return score

          def deepseek_summarize(title, summary, link):
              # ä½ å’Œä½ è€å©†çš„â€œç»Ÿä¸€è¾“å‡ºæ ¼å¼â€ï¼šä¸å¸¦ Markdown
              system_prompt = (
                  "ä½ æ˜¯ä¸€ä½é¢å‘å®æ“ç”¨æˆ·çš„AIæŠ€æœ¯æƒ…æŠ¥å®˜ã€‚"
                  "è¯·æŠŠå†…å®¹å‹ç¼©æˆå¯æ‰§è¡Œæ‘˜è¦ï¼Œä¸¥ç¦ä½¿ç”¨ä»»ä½•Markdownç¬¦å·ï¼ˆå¦‚# * - ç­‰ï¼‰ã€‚"
                  "è¾“å‡ºç»“æ„å›ºå®šä¸º5è¡Œï¼š"
                  "1) æ ‡é¢˜ï¼šä¸€å¥ä¸­æ–‡"
                  "2) è¿™æ˜¯ä»€ä¹ˆï¼šå·¥å…·/è¯¾ç¨‹/æ–¹æ³•/æ›´æ–°ï¼ˆ20å­—å†…ï¼‰"
                  "3) ä¸»è¦å†…å®¹ï¼šç”¨ä¸è¶…è¿‡40å­—æ¦‚æ‹¬å®ƒåœ¨è®²ä»€ä¹ˆ"
                  "4) èƒ½è§£å†³ä»€ä¹ˆé—®é¢˜ï¼šæ˜ç¡®æŒ‡å‡ºèƒ½è§£å†³çš„å…·ä½“ç—›ç‚¹ï¼ˆ40å­—å†…ï¼‰"
                  "5) ä¸‹ä¸€æ­¥ï¼šç»™å‡º1ä¸ªç«‹åˆ»å¯åšçš„åŠ¨ä½œï¼ˆ25å­—å†…ï¼‰"
              )

              user_content = f"Title: {title}\nSummary: {summary}\nLink: {link}"

              resp = requests.post(
                  "https://api.deepseek.com/chat/completions",
                  headers={"Authorization": f"Bearer {api_key}"},
                  json={
                      "model": "deepseek-chat",
                      "messages": [
                          {"role": "system", "content": system_prompt},
                          {"role": "user", "content": user_content},
                      ],
                      "temperature": 0.2,
                  },
                  timeout=35,
              )
              if resp.status_code != 200:
                  return f"æ ‡é¢˜ï¼š{title}\nè¿™æ˜¯ä»€ä¹ˆï¼šAIå†…å®¹\nè§£å†³ä»€ä¹ˆï¼šæ¥å£å¼‚å¸¸\né€‚åˆè°ï¼šç”µå•†/æ•™å­¦ï¼šæœªçŸ¥\nä¸‹ä¸€æ­¥ï¼šç¨åé‡è¯•"
              data = resp.json()
              txt = data["choices"][0]["message"]["content"].strip()

              # å†æ¸…æ´—ä¸€æ¬¡ï¼Œé˜²æ­¢æ¨¡å‹å·è·‘ç¬¦å·
              txt = txt.replace("**", "").replace("###", "").replace("#", "").replace("`", "")
              txt = txt.replace("â€¢", "").replace("Â·", "")
              return txt

          sent_links = load_history()
          print(f"å·²åŠ è½½å†å²é“¾æ¥ï¼š{len(sent_links)}")

          candidates = []
          print("å¼€å§‹æŠ“å–èµ„è®¯...")

          for url in SOURCES:
              try:
                  resp = requests.get(url, headers=HEADERS, timeout=20)
                  feed = feedparser.parse(resp.text)
                  if not getattr(feed, "entries", None):
                      continue

                  # æ¯ä¸ªæºæŠ“æ›´å¤šï¼Œé æ‰“åˆ†ç­›
                  for entry in feed.entries[:40]:
                      title = getattr(entry, "title", "") or ""
                      link = getattr(entry, "link", "") or ""
                      summary_html = entry.get("summary", "") or ""
                      summary = clean_text(summary_html)[:1600]

                      if not link:
                          continue

                      # å»é‡ï¼šlink å»é‡
                      if link in sent_links:
                          continue

                      # é¢å¤–å»é‡ï¼šåŒæ ‡é¢˜+æº çš„ hashï¼ˆé˜²æŸäº›æº link å˜åŒ–ï¼‰
                      h = hashlib.sha1((title + "|" + url).encode("utf-8")).hexdigest()
                      if h in sent_links:
                          continue

                      sc = score_entry(title, summary, link, url)

                      # åŸºç¡€é—¨æ§›ï¼šè‡³å°‘ä¸ä¸‰ç±»å…³é”®è¯ä¹‹ä¸€æœ‰å…³ï¼Œä¸”ä¸æ˜¯çº¯å™ªéŸ³
                      text = normalize(title + " " + summary)
                      if sc < 2.5:
                          # å…è®¸å®˜æ–¹æºæ”¾å®½ä¸€ç‚¹ï¼Œä½†ä»è¦åŸºæœ¬ç›¸å…³
                          if ("openai.com" not in url and "huggingface.co" not in url and "langchain.dev" not in url):
                              continue
                          if not (contains_any(text, GENERAL_KEYWORDS) or contains_any(text, ECOM_KEYWORDS) or contains_any(text, TEACH_KEYWORDS)):
                              continue

                      candidates.append({
                          "score": sc,
                          "title": title,
                          "summary": summary,
                          "link": link,
                          "source": url,
                          "hash": h,
                      })
              except Exception as e:
                  print(f"æŠ“å–å¤±è´¥ {url}: {e}")

          if not candidates:
              print("æ²¡æœ‰å‘ç°å¯æ¨é€çš„æ–°å†…å®¹ï¼ˆæˆ–éƒ½å·²æ¨è¿‡ï¼‰ï¼Œç»“æŸè¿è¡Œã€‚")
              sys.exit(0)

          # æœ€é«˜åˆ†ä¼˜å…ˆ
          candidates.sort(key=lambda x: x["score"], reverse=True)

          # æ¯å¤©æ¨é€æ¡æ•°ï¼šå»ºè®® 4ï¼ˆä¿¡æ¯é‡åˆšå¥½ï¼‰
          PICK_N = 4
          picked = candidates[:PICK_N]

          # ç»„è£…æ¶ˆæ¯
          today_jst = datetime.now(timezone.utc).astimezone().strftime("%Y-%m-%d")
          final_message = f"ğŸ§  AI æ¯æ—¥ç®€æŠ¥ï¼ˆç”µå•†xæ—¥è¯­æ•™å­¦ï¼‰\næ—¥æœŸï¼š{today_jst}\n\n"

          for idx, c in enumerate(picked, 1):
              title_en = c["title"]
              summary_en = c["summary"][:550]
              link = c["link"]

              try:
                  ai_result = deepseek_summarize(title_en, summary_en, link)
              except Exception as e:
                  ai_result = f"æ ‡é¢˜ï¼š{title_en}\nè¿™æ˜¯ä»€ä¹ˆï¼šAIå†…å®¹\nè§£å†³ä»€ä¹ˆï¼šæ€»ç»“å¤±è´¥\né€‚åˆè°ï¼šç”µå•†/æ•™å­¦ï¼šæœªçŸ¥\nä¸‹ä¸€æ­¥ï¼šç¨åé‡è¯•ï¼ˆ{e})"

              final_message += f"ã€{idx}ã€‘\n{ai_result}\né“¾æ¥ï¼š{link}\n\n----------------\n\n"

              # è®°å½•å·²æ¨é€ï¼šlink + hash éƒ½è®°ï¼ŒåŒä¿é™©
              sent_links.add(link)
              sent_links.add(c["hash"])

              # å°ç¡ä¸€ä¸‹ï¼Œé¿å…æ¥å£å¶å‘é™æµ
              time.sleep(1.2)

          # ä¿å­˜å†å²
          save_history(sent_links)

          # æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº
          try:
              requests.post(webhook, json={"msgtype": "text", "text": {"content": final_message}}, timeout=12)
              print("æ¨é€å®Œæˆï¼")
          except Exception as e:
              print(f"æ¨é€å¤±è´¥: {e}")
              sys.exit(1)
          EOF

      - name: Commit sent history
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add sent_history.json
          git commit -m "Update sent history" || echo "No changes to commit"
          git push
